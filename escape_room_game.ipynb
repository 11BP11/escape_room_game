{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escape Room Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn import tree\n",
    "import emoji "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid state: (7, 2, 0, 0, 2)\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | ğŸ”‘ |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    | ğŸ¤  |   \n",
      "\n",
      "\n",
      "Possible moves: â†‘(0) â†’(1) â†“(2) â†(3)\n",
      "\n",
      "\n",
      "Standard grid:\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    |    |    | ğŸšª |   \n",
      "  --------------------------------------------\n",
      "1     | ğŸ¤  |    | ğŸ”¥ |    | ğŸ”‘ |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    |    |   \n"
     ]
    }
   ],
   "source": [
    "class Grid:\n",
    "    def __init__(self, pos_init=(1, 1), key_init=1, door_init=0):\n",
    "        self.pos_init = pos_init\n",
    "        self.key_init = key_init\n",
    "        self.key_pos = (5, key_init)\n",
    "        self.door_init = door_init\n",
    "        self.door_pos = (7, door_init)\n",
    "        self.pos = list(pos_init)\n",
    "        self.has_key = False\n",
    "        self.move_count = 0\n",
    "        self.fires = [(3, 1), (3, 2)]\n",
    "        self.score = 0\n",
    "        self.move_reward = -1\n",
    "        self.fire_reward = -50\n",
    "        self.key_reward = 0\n",
    "        self.escape_reward = 0\n",
    "        self.max_nrof_moves = 25\n",
    "        self.reward_functions = {}\n",
    "        \n",
    "        self.all_states_list = None\n",
    "        \n",
    "        if self.pos == self.key_pos:\n",
    "            self.has_key = True\n",
    "    \n",
    "    def from_state(self, state):\n",
    "        posX, posY, has_key, key_init, door_init = state\n",
    "        self.__init__((posX, posY), key_init, door_init)\n",
    "        self.has_key = has_key\n",
    "        return self\n",
    "    \n",
    "    def make_random(self):\n",
    "        x = random.randrange(9)\n",
    "        y = random.randrange(3)\n",
    "        k = random.randrange(3)\n",
    "        d = random.randrange(3)\n",
    "        # print(\"Initial parameters:\", x, y, k, d)\n",
    "        self.__init__((x, y), k, d)\n",
    "        return self\n",
    "    \n",
    "    def add_key(self, remove=False):\n",
    "        self.has_key = not remove\n",
    "        return self\n",
    "    \n",
    "    def as_list(self, with_emoji=True):\n",
    "        grid_list = []\n",
    "        for i in range(9):\n",
    "            grid_list.append([])\n",
    "            for j in range(3):\n",
    "                if (i, j) == tuple(self.pos):\n",
    "                    # field = \"A\"\n",
    "                    field = emoji.emojize(\":cowboy_hat_face:\") if with_emoji else \"A\"\n",
    "                elif (i, j) in self.fires:\n",
    "                    # field = \"F\"\n",
    "                    field = emoji.emojize(\":fire:\") if with_emoji else \"F\"\n",
    "                elif (i, j) == self.key_pos:\n",
    "                    # field = \"K\" if not self.has_key else \".\"\n",
    "                    if self.has_key:\n",
    "                        field = \". \"\n",
    "                    else:\n",
    "                        field = emoji.emojize(\":key:\") if with_emoji else \"K\"\n",
    "                elif (i, j) == self.door_pos:\n",
    "                    # field = \"D\"\n",
    "                    field = emoji.emojize(\":door:\") if with_emoji else \"D\"\n",
    "                else:\n",
    "                    field = \"  \"\n",
    "                grid_list[i].append(field)\n",
    "        return grid_list\n",
    "    \n",
    "    def __str__(self, grid_list=None):\n",
    "        if grid_list is None:\n",
    "            grid_list = self.as_list()\n",
    "        conlen = len(grid_list[0][0]) + 2\n",
    "        row_strs = []\n",
    "        for j in range(3):\n",
    "            row = [grid_list[i][j] for i in range(9)]\n",
    "            row_str = str(j) + \"  \" + \" | \".join(row)\n",
    "            row_strs.append(row_str)\n",
    "        grid_row = \"\\n  \" + \"-\"*(conlen*9+8) + \"\\n\"\n",
    "        firstline = \"  \"\n",
    "        firstline += \" \"*(conlen//2)\n",
    "        firstline += (\" \"*conlen).join([str(i) for i in range(9)]) + \" \\n\"\n",
    "        grid_str = firstline + grid_row.join(row_strs)\n",
    "        return grid_str\n",
    "    \n",
    "    def move(self, direction, suppress_print=False):\n",
    "        # redo using next state\n",
    "        if direction == 0:\n",
    "            self.pos[1] = max(0, self.pos[1]-1)\n",
    "        elif direction == 1:\n",
    "            self.pos[0] = min(8, self.pos[0]+1)\n",
    "        elif direction == 2:\n",
    "            self.pos[1] = min(2, self.pos[1]+1)\n",
    "        elif direction == 3:\n",
    "            self.pos[0] = max(0, self.pos[0]-1)\n",
    "        else:\n",
    "            print(\"\\nInvalid move!! (pos {}, direction {})\\n\".format(self.pos, direction))\n",
    "            \n",
    "        self.move_count += 1\n",
    "        self.score += self.move_reward\n",
    "        \n",
    "        if tuple(self.pos) == self.key_pos and not self.has_key:\n",
    "            self.has_key = True\n",
    "            self.score += self.key_reward\n",
    "            if not suppress_print: print(\"Found key!\")\n",
    "            \n",
    "        if tuple(self.pos) in self.fires:\n",
    "            self.score += self.fire_reward\n",
    "            if not suppress_print: print(\"Oouch, you got burned!\")\n",
    "            \n",
    "        if tuple(self.pos) == self.door_pos and self.has_key:\n",
    "            self.score += self.escape_reward\n",
    "            if not suppress_print: print(\"Found door! (finished in \" + str(self.move_count) + \" moves.)\")\n",
    "            if not suppress_print: print(\"Congratulations! Achieved a score of \" + str(self.score) + \".\")\n",
    "            return False\n",
    "        \n",
    "        if self.move_count >= self.max_nrof_moves:\n",
    "            if not suppress_print: print(\"You loose! (move \" + str(self.move_count) +\")\")\n",
    "            if not suppress_print: print(\"Ended on a score of\", self.score)\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def next_state(self, state, direction):\n",
    "        posX, posY, has_key, key_init, door_init = state\n",
    "        reward = self.move_reward\n",
    "        finished = False\n",
    "        \n",
    "        if direction == 0:\n",
    "            posY = max(0, posY-1)\n",
    "        elif direction == 1:\n",
    "            posX = min(8, posX+1)\n",
    "        elif direction == 2:\n",
    "            posY = min(2, posY+1)\n",
    "        elif direction == 3:\n",
    "            posX = max(0, posX-1)\n",
    "        else:\n",
    "            print(\"\\nInvalid move!! (state {}, direction {})\\n\".format(state, direction))\n",
    "            return None\n",
    "        \n",
    "        if (posX, posY) == (self.key_pos[0], key_init) and not has_key:\n",
    "            has_key = True\n",
    "            reward += self.key_reward\n",
    "            \n",
    "        if (posX, posY) in self.fires:\n",
    "            reward += self.fire_reward\n",
    "            \n",
    "        if (posX, posY) == (self.door_pos[0], door_init) and has_key:\n",
    "            reward += self.escape_reward\n",
    "            finished = True\n",
    "                    \n",
    "        state = posX, posY, has_key, key_init, door_init\n",
    "        return state, reward, finished\n",
    "\n",
    "    def oracle_move(self, state=None):\n",
    "        if state is None:\n",
    "            x, y = self.pos\n",
    "            has_key = int(self.has_key)\n",
    "            key_init = self.key_init\n",
    "            door_init = self.door_init\n",
    "        else:\n",
    "            x, y, has_key, key_init, door_init = state\n",
    "            \n",
    "        if x<=4:\n",
    "            if x<3 and y>0:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        elif has_key==1:\n",
    "            if x<=6:\n",
    "                return 1\n",
    "            elif x >7:\n",
    "                return 3\n",
    "            elif y==0:\n",
    "                return 2\n",
    "            elif y==2:\n",
    "                return 0\n",
    "            elif door_init==0:\n",
    "                return 0\n",
    "            elif door_init==2:\n",
    "                return 2\n",
    "            else:\n",
    "                return None\n",
    "        elif x>5:\n",
    "            return 3\n",
    "        elif y==0:\n",
    "            return 2\n",
    "        elif y==2:\n",
    "            return 0\n",
    "        elif key_init==0:\n",
    "            return 0\n",
    "        elif key_init==2:\n",
    "            return 2\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    @property\n",
    "    def state(self):\n",
    "        return(self.pos[0], self.pos[1], int(self.has_key), self.key_init, self.door_init)\n",
    "    \n",
    "    def get_state_str(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.state\n",
    "    \n",
    "        state_str = \"Agent position: ({}, {}), has key: {}, key and door positions: {}, {}\".format(*state)\n",
    "        return state_str\n",
    "    state_str = property(get_state_str)\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def all_states(self):\n",
    "        if self.all_states_list is not None:\n",
    "            return self.all_states_list\n",
    "        else:\n",
    "            maxnrof_elements = [9,3,2,3,3]\n",
    "            curr_state = [i-1 for i in maxnrof_elements]\n",
    "            all_states = []\n",
    "            while curr_state != [0,0,0,0,0]:\n",
    "                all_states.append([i for i in curr_state])\n",
    "\n",
    "                for i in range(len(maxnrof_elements)):\n",
    "                    if curr_state[i] == 0:\n",
    "                        curr_state[i] = maxnrof_elements[i] - 1\n",
    "                    else:\n",
    "                        curr_state[i] = curr_state[i] - 1\n",
    "                        break\n",
    "            all_states.append([0,0,0,0,0])\n",
    "            \n",
    "            assert len(all_states) == 9*3*2*3*3\n",
    "            self.all_states_list = [tuple(state) for state in all_states]\n",
    "            return self.all_states_list\n",
    "\n",
    "    def print_fn_on_grid(self, fn, fn_name=\"Policy\"):\n",
    "        \"\"\"\n",
    "        fn: state -> string (of constant length)\n",
    "        \"\"\"\n",
    "        grid_list = self.as_list()\n",
    "        for state in self.all_states:\n",
    "            if state[2:] != self.state[2:]:\n",
    "                continue\n",
    "            else:\n",
    "                grid_list[state[0]][state[1]] = fn(grid_list[state[0]][state[1]], state)\n",
    "        print(\"\\n\" + fn_name + \" without key:\")\n",
    "        print(self.__str__(grid_list=grid_list))\n",
    "        gl_no_key = grid_list\n",
    "\n",
    "        self.has_key=True\n",
    "        grid_list = self.as_list()\n",
    "        for state in self.all_states:\n",
    "            if state[2:] != self.state[2:]:\n",
    "                continue\n",
    "            else:\n",
    "                grid_list[state[0]][state[1]] = fn(grid_list[state[0]][state[1]], state)\n",
    "        print(\"\\n\" + fn_name + \" with key:\")\n",
    "        print(self.__str__(grid_list=grid_list))\n",
    "        \n",
    "        return gl_no_key, grid_list        \n",
    "\n",
    "    def print_policy(self, policy, policy_name=\"Policy\"):\n",
    "        def fn(grid_symb, state):\n",
    "            arrow = dir_str(policy(state))\n",
    "            if grid_symb == \" \":\n",
    "                return \" \" + arrow + \" \"\n",
    "            else:\n",
    "                return grid_symb + \" \" + arrow\n",
    "            \n",
    "        return self.print_fn_on_grid(fn, fn_name=policy_name)  \n",
    "     \n",
    "    def print_reward_fn(self, reward_fn, show_diff=False, small=False, nr_len=3):\n",
    "        assert not self.has_key\n",
    "        def reward_str(grid_symb, state):\n",
    "            rew = reward_fn(state)\n",
    "            opt_rew = self.reward_function(state) if show_diff else 0\n",
    "            rew_str = \" ?\"+\" \"*(nr_len-2) if rew is None else (\"{: \"+str(nr_len)+\"d}\").format(rew-opt_rew)\n",
    "            \n",
    "            if not small:\n",
    "                if grid_symb == \" \"*len(grid_symb):\n",
    "                    return \" \"*len(grid_symb) + rew_str + \" \"\n",
    "                else:\n",
    "                    return grid_symb + \" \" + rew_str                \n",
    "            else:\n",
    "                if grid_symb == \" \"*len(grid_symb):\n",
    "                    return rew_str \n",
    "                else:\n",
    "                    return grid_symb + \" \"*(nr_len-2)\n",
    "\n",
    "        fn_name = \"Future reward\" if not show_diff else \"Relative future reward\"\n",
    "        return self.print_fn_on_grid(reward_str, fn_name=fn_name);\n",
    "\n",
    "        \n",
    "    def get_reward_function(self, policy=None, suppress_print=True):\n",
    "        if policy is None:\n",
    "            policy = self.oracle_move\n",
    "        \n",
    "        if policy in self.reward_functions.keys():\n",
    "            return self.reward_functions[policy]\n",
    "        \n",
    "        # states_to_process = [tuple(state) for state in self.all_states]\n",
    "        states_to_process = [(tuple(state), moves_left) for state in self.all_states \\\n",
    "                             for moves_left in range(self.max_nrof_moves+1)]\n",
    "        reward = {state:None for state in states_to_process}\n",
    "        # stats = {state:None for state in states_to_process} # (#fire, key?, door?)\n",
    "        if not suppress_print: print(\"Starting going through \" + str(len(states_to_process)) + \" states.\")\n",
    "\n",
    "        count = 0\n",
    "        first_new = len(states_to_process)\n",
    "        epoch_nr = 1\n",
    "        rew_ass_list = []\n",
    "        impossible_states, end_states = [], []\n",
    "        while len(states_to_process) > 0:\n",
    "            if count==first_new:\n",
    "                if not suppress_print: print(\"Went through epoch {}. Still got {} to process.\".format(\n",
    "                    epoch_nr, len(states_to_process)))\n",
    "                epoch_nr += 1\n",
    "                first_new = count + len(states_to_process)\n",
    "\n",
    "            state, moves_left = states_to_process.pop(0) # moves_left .. Number of moves left before time is over.\n",
    "            if moves_left == 0:\n",
    "                reward[(state, moves_left)] = 0\n",
    "                continue\n",
    "            move = policy(state)\n",
    "\n",
    "            if state[0] == 5 and state[1]==state[3] and state[2]==0: # On key field but no key.\n",
    "                impossible_states.append(state)\n",
    "                continue\n",
    "            if state[0] == 7 and state[1]==state[4] and state[2]==1: # end states\n",
    "                end_states.append(state)\n",
    "                reward[(state, moves_left)] = 0\n",
    "                continue\n",
    "\n",
    "            if move is None:\n",
    "                print(\"Don't know how to move :0\")\n",
    "                print(\"State:\", state)\n",
    "            next_state, move_reward, finished = self.next_state(state, move)\n",
    "\n",
    "            if finished:\n",
    "                # reward[next_state] = 0\n",
    "                reward[(next_state, moves_left)] = 0\n",
    "            # if reward[next_state] is not None:\n",
    "            if reward[(next_state, moves_left-1)] is not None:\n",
    "                # reward[state] = reward[next_state] + move_reward\n",
    "                reward[(state, moves_left)] = reward[(next_state, moves_left-1)] + move_reward\n",
    "                # rew_ass_list.append(reward[state])\n",
    "                rew_ass_list.append(reward[(state, moves_left)])\n",
    "            else:\n",
    "                # states_to_process.append(state)\n",
    "                states_to_process.append((state, moves_left))\n",
    "\n",
    "            if count == 1000000:\n",
    "                print(\"\\nCba!!!!\\n\")\n",
    "                break\n",
    "            count += 1\n",
    "\n",
    "        reward_dict = reward\n",
    "        def reward(state):\n",
    "            # return reward_dict[tuple(state)]\n",
    "            return reward_dict[(state, self.max_nrof_moves)]\n",
    "\n",
    "        if not suppress_print: print(\"\\nImpossible states:\", set(impossible_states))\n",
    "        if not suppress_print: print(\"\\nEnd states:\", set(end_states))\n",
    "        \n",
    "        self.reward_functions[policy] = reward\n",
    "        return reward\n",
    "    reward_function = property(get_reward_function)\n",
    "     \n",
    "    def get_average_reward(self, policy=None, suppress_print=True):\n",
    "        reward_fn = self.get_reward_function(policy=policy, suppress_print=suppress_print)\n",
    "        \n",
    "        rew_sum = 0\n",
    "        for posX in range(9):\n",
    "            for posY in range(3):\n",
    "                has_key = 0\n",
    "                if (posX, posY) == Grid().key_pos:\n",
    "                    has_key = 1\n",
    "                rew = reward_fn((posX, posY, has_key, 1, 0))\n",
    "                rew_sum += rew\n",
    "        \n",
    "        # print(\"\\nAverage reward of {:.2f}.\".format(rew_sum/9/3))\n",
    "        return(rew_sum/9/3)\n",
    "    average_reward = property(get_average_reward)\n",
    "        \n",
    "    def generate_data(self, policy=None, oracle=None, nrof_rollouts=1000, fixed_pos=False, fixed_key=True, fixed_door=True):\n",
    "        if policy is None:\n",
    "            policy = self.oracle_move\n",
    "        if oracle is None:\n",
    "            oracle = self.oracle_move\n",
    "        \n",
    "        (x, y), k, d = self.pos, self.key_init, self.door_init\n",
    "\n",
    "        xs = []\n",
    "        ys = []\n",
    "\n",
    "        for _ in range(nrof_rollouts):\n",
    "            if not fixed_pos:\n",
    "                x = random.randrange(9)\n",
    "                y = random.randrange(3)\n",
    "            if not fixed_key:\n",
    "                k = random.randrange(3)\n",
    "            if not fixed_door:\n",
    "                d = random.randrange(3)\n",
    "            grid = Grid((x, y), k, d)\n",
    "\n",
    "            oracle_move = oracle(state=grid.state)\n",
    "            policy_move = policy(grid.state)\n",
    "\n",
    "            if oracle_move is None: # = finished\n",
    "                continue\n",
    "\n",
    "            xs.append(grid.state)\n",
    "            ys.append(oracle_move)\n",
    "\n",
    "            while grid.move(policy_move, suppress_print=True):\n",
    "                oracle_move = oracle(state=grid.state)\n",
    "                policy_move = policy(grid.state)\n",
    "                if oracle_move is None:\n",
    "                    print(grid.state)\n",
    "                    print(grid)\n",
    "                    assert False\n",
    "                xs.append(grid.state)\n",
    "                ys.append(oracle_move)\n",
    "\n",
    "        return xs, ys\n",
    "    \n",
    "grid = Grid().make_random()\n",
    "print(\"Grid state:\", grid.state)\n",
    "print(grid)\n",
    "\n",
    "def dir_str(direction):\n",
    "    if direction == 0:\n",
    "        return \"\\u2191\"\n",
    "    elif direction == 1:\n",
    "        return \"\\u2192\"\n",
    "    elif direction == 2:\n",
    "        return \"\\u2193\"\n",
    "    elif direction == 3:\n",
    "        return \"\\u2190\"\n",
    "    else:\n",
    "        return \"?\"\n",
    "\n",
    "mo_st = \"\\n\\nPossible moves: \" + \" \".join([dir_str(i)+\"(\"+str(i)+\")\" for i in range(4)])\n",
    "print(mo_st)\n",
    "\n",
    "print(\"\\n\\nStandard grid:\")\n",
    "print(Grid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing oracle on following grid:\n",
      "\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | ğŸ”‘ |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ¤  |    |    |    | ğŸšª |    \n",
      "\n",
      "Move 1:\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | ğŸ”‘ |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ | ğŸ¤  |    |    | ğŸšª |    \n",
      "\n",
      "\n",
      "Move 2:\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | ğŸ”‘ |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    | ğŸ¤  |    | ğŸšª |    \n",
      "\n",
      "\n",
      "Move 3:\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | ğŸ”‘ |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    | ğŸ¤  |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    | ğŸšª |    \n",
      "\n",
      "\n",
      "Found key!\n",
      "Move 4:\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | ğŸ¤  |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    | ğŸšª |    \n",
      "\n",
      "\n",
      "Move 5:\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | .  | ğŸ¤  |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    | ğŸšª |    \n",
      "\n",
      "\n",
      "Move 6:\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | .  |    | ğŸ¤  |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    | ğŸšª |    \n",
      "\n",
      "\n",
      "Move 7:\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | .  |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    |    |    | ğŸ¤  |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    | ğŸšª |    \n",
      "\n",
      "\n",
      "Found door! (finished in 8 moves.)\n",
      "Congratulations! Achieved a score of -8.\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | .  |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    | ğŸ¤  |   \n"
     ]
    }
   ],
   "source": [
    "print(\"Showing oracle on following grid:\\n\")\n",
    "\n",
    "grid = Grid().make_random()\n",
    "print(grid, \"\\n\")\n",
    "\n",
    "while grid.move(grid.oracle_move()):\n",
    "    print(\"Move \" + str(grid.move_count) + \":\")\n",
    "    print(grid, \"\\n\\n\")\n",
    "    \n",
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing policy that always goes right:\n",
      "\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "1     | ğŸ¤  |    | ğŸ”¥ |    | ğŸ”‘ |    | ğŸšª |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    |    |    \n",
      "\n",
      "\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    | ğŸ¤  | ğŸ”¥ |    | ğŸ”‘ |    | ğŸšª |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    |    |    \n",
      "\n",
      "\n",
      "Oouch, you got burned!\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ¤  |    | ğŸ”‘ |    | ğŸšª |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    |    |    \n",
      "\n",
      "\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ | ğŸ¤  | ğŸ”‘ |    | ğŸšª |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    |    |    \n",
      "\n",
      "\n",
      "Found key!\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    | ğŸ¤  |    | ğŸšª |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    |    |    \n",
      "\n",
      "\n",
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    |    |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ |    | .  | ğŸ¤  | ğŸšª |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    |    |    \n",
      "\n",
      "\n",
      "Found door! (finished in 6 moves.)\n",
      "Congratulations! Achieved a score of -56.\n"
     ]
    }
   ],
   "source": [
    "print(\"Showing policy that always goes right:\\n\")\n",
    "grid = Grid((1, 1), 1, 1)\n",
    "print(grid, \"\\n\\n\")\n",
    "\n",
    "while grid.move(1):\n",
    "    print(grid, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing (hard coded) oracle policy:\n",
      "\n",
      "\n",
      "Oracle policy without key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†“ |    â† | ğŸšª â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "1     â†‘ | ğŸ¤  â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ | ğŸ”‘ ? |    â† |    â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "2     â†‘ |    â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ |    â†‘ |    â† |    â† |    â†\n",
      "\n",
      "Oracle policy with key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ | ğŸšª â†“ |    â†\n",
      "  --------------------------------------------------------------\n",
      "1     â†‘ | ğŸ¤  â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ | .  â†’ |    â†’ |    â†‘ |    â†\n",
      "  --------------------------------------------------------------\n",
      "2     â†‘ |    â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ |    â†’ |    â†’ |    â†‘ |    â†\n"
     ]
    }
   ],
   "source": [
    "print(\"Showing (hard coded) oracle policy:\\n\")\n",
    "grid = Grid()\n",
    "# print(grid)\n",
    "policy = grid.oracle_move\n",
    "grid.print_policy(policy, policy_name=\"Oracle policy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reward function for each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Future reward without key:\n",
      "    0     1     2     3     4     5     6     7     8 \n",
      "0   -9 |  -8 |  -7 |  -6 |  -5 |  -4 |  -5 | ğŸšª  |  -7\n",
      "  -----------------------------------------------------\n",
      "1  -10 | ğŸ¤   |  -8 | ğŸ”¥  |  -4 | ğŸ”‘  |  -4 |  -5 |  -6\n",
      "  -----------------------------------------------------\n",
      "2  -11 | -10 |  -9 | ğŸ”¥  |  -5 |  -4 |  -5 |  -6 |  -7\n",
      "\n",
      "Future reward with key:\n",
      "    0     1     2     3     4     5     6     7     8 \n",
      "0   -7 |  -6 |  -5 |  -4 |  -3 |  -2 |  -1 | ğŸšª  |  -1\n",
      "  -----------------------------------------------------\n",
      "1   -8 | ğŸ¤   |  -6 | ğŸ”¥  |  -4 | .   |  -2 |  -1 |  -2\n",
      "  -----------------------------------------------------\n",
      "2   -9 |  -8 |  -7 | ğŸ”¥  |  -5 |  -4 |  -3 |  -2 |  -3\n"
     ]
    }
   ],
   "source": [
    "reward = grid.reward_function\n",
    "\n",
    "# grid = Grid().make_random()\n",
    "grid = Grid()\n",
    "# print(\"\\n\\nShowing reward function for following grid:\")\n",
    "# print(grid)\n",
    "\n",
    "grid.add_key(remove=True).print_reward_fn(reward, small=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0    1    2    3    4    5    6    7    8 \n",
      "0     |    |    |    |    | ğŸ”‘ |    |    |   \n",
      "  --------------------------------------------\n",
      "1     |    |    | ğŸ”¥ | ğŸ¤  |    |    |    |   \n",
      "  --------------------------------------------\n",
      "2     |    |    | ğŸ”¥ |    |    |    | ğŸšª |   \n",
      "Agent position: (4, 1), has key: 0, key and door positions: 0, 2\n",
      "Reward(state): -6 \n",
      "\n",
      "Oracle move: â†’ \n",
      "\n",
      "Q(state, â†‘) = -6\n",
      "Q(state, â†’) = -6\n",
      "Q(state, â†“) = -8\n",
      "Q(state, â†) = -58\n"
     ]
    }
   ],
   "source": [
    "Q_fun_dict = {}\n",
    "for state in grid.all_states:\n",
    "    for direction in range(4):\n",
    "        next_state, move_reward, finished = grid.next_state(state, direction)\n",
    "        Q_fun_dict[(state, direction)] = reward(next_state) + move_reward\n",
    "        \n",
    "        \n",
    "def Q_fn(state, direction):\n",
    "    return Q_fun_dict[(state, direction)]\n",
    "        \n",
    "grid = Grid().make_random()\n",
    "# grid = Grid().make_random().add_key()\n",
    "print(grid)\n",
    "print(grid.state_str)\n",
    "print(\"Reward(state):\", reward(grid.state), \"\\n\")\n",
    "print(\"Oracle move:\", dir_str(grid.oracle_move()), \"\\n\")\n",
    "\n",
    "for direction in range(4):\n",
    "    print(\"Q(state, {}) = {}\".format(dir_str(direction), Q_fn(grid.state, direction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a decision tree\n",
    "## 1) Imitation Learning:\n",
    "Generate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 63541 data points.\n"
     ]
    }
   ],
   "source": [
    "xs, ys = Grid().generate_data(nrof_rollouts=10000)\n",
    "\n",
    "print(\"Collected {} data points.\".format(len(xs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy without key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†“ |    â† | ğŸšª â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "1     â†’ | ğŸ¤  â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ | ğŸ”‘ â†“ |    â† |    â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "2     â†’ |    â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ |    â†“ |    â† |    â† |    â†\n",
      "\n",
      "Policy with key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ | ğŸšª â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "1     â†’ | ğŸ¤  â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ | .  â†’ |    â†’ |    â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "2     â†’ |    â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ |    â†’ |    â†’ |    â†‘ |    â†‘\n"
     ]
    }
   ],
   "source": [
    "max_leaf_nodes = 6\n",
    "clf_imitation = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\n",
    "clf_imitation = clf_imitation.fit(xs, ys)\n",
    "nodes = []\n",
    "# nodes = tree.plot_tree(clf_imidation)\n",
    "\n",
    "imitation_policy = lambda state: clf_imitation.predict([state])[0]\n",
    "Grid().print_policy(imitation_policy);\n",
    "\n",
    "node_text = \"\\nNodes:\\n\"\n",
    "node_fls = [node.get_text().split(\"\\n\")[0] for node in nodes]\n",
    "node_text += \"\\n\".join([node_fl for node_fl in node_fls if node_fl[0] != \"g\"])\n",
    "# print(node_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imitation policy without key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†“ |    â† | ğŸšª â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "1     â†’ | ğŸ¤  â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ | ğŸ”‘ â†“ |    â† |    â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "2     â†’ |    â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ |    â†“ |    â† |    â† |    â†\n",
      "\n",
      "Imitation policy with key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ | ğŸšª â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "1     â†’ | ğŸ¤  â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ | .  â†’ |    â†’ |    â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "2     â†’ |    â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ |    â†’ |    â†’ |    â†‘ |    â†‘\n",
      "\n",
      "Relative future reward without key:\n",
      "      0         1         2         3         4         5         6         7         8 \n",
      "0       0  |      0  |      0  |      0  |      0  |      0  |      0  | ğŸšª    0 |      0 \n",
      "  -----------------------------------------------------------------------------------------\n",
      "1     -48  | ğŸ¤   -48 |    -48  | ğŸ”¥    0 |      0  | ğŸ”‘  ?   |      0  |      0  |      0 \n",
      "  -----------------------------------------------------------------------------------------\n",
      "2     -64  |    -65  |    -66  | ğŸ”¥  -19 |    -20  |    -21  |    -20  |    -19  |    -18 \n",
      "\n",
      "Relative future reward with key:\n",
      "      0         1         2         3         4         5         6         7         8 \n",
      "0       0  |      0  |      0  |      0  |      0  |      0  |      0  | ğŸšª    0 |    -24 \n",
      "  -----------------------------------------------------------------------------------------\n",
      "1     -50  | ğŸ¤   -50 |    -50  | ğŸ”¥    0 |      0  | .     0 |      0  |      0  |    -23 \n",
      "  -----------------------------------------------------------------------------------------\n",
      "2     -50  |    -50  |    -50  | ğŸ”¥    0 |      0  |      0  |      0  |      0  |    -22 \n",
      "\n",
      "\n",
      "Average reward: -23.33 (optimal: -6.44).\n"
     ]
    }
   ],
   "source": [
    "rew_grid = Grid()\n",
    "imitation_reward_fn = rew_grid.get_reward_function(policy=imitation_policy, suppress_print=True)\n",
    "\n",
    "grid = Grid()\n",
    "grid.print_policy(imitation_policy, policy_name=\"Imitation policy\");\n",
    "rew_grid.add_key(remove=True).print_reward_fn(imitation_reward_fn, nr_len=4, show_diff=True)\n",
    "\n",
    "average_reward = rew_grid.get_average_reward(policy=imitation_policy, suppress_print=True)\n",
    "optimal_reward = rew_grid.average_reward\n",
    "print(\"\\n\\nAverage reward: {:.2f} (optimal: {:.2f}).\".format(average_reward, optimal_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dagger\n",
    "Firstly, train on oracle data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 6294 data points.\n",
      "\n",
      "Policy without key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†“ |    â† | ğŸšª â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "1     â†’ | ğŸ¤  â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ | ğŸ”‘ â†“ |    â† |    â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "2     â†’ |    â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ |    â†“ |    â† |    â† |    â†\n",
      "\n",
      "Policy with key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ | ğŸšª â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "1     â†’ | ğŸ¤  â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ | .  â†’ |    â†’ |    â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "2     â†’ |    â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ |    â†’ |    â†’ |    â†‘ |    â†‘\n",
      "\n",
      "Relative future reward without key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0      0  |     0  |     0  |     0  |     0  |     0  |     0  | ğŸšª   0 |     0 \n",
      "  --------------------------------------------------------------------------------\n",
      "1    -48  | ğŸ¤  -48 |   -48  | ğŸ”¥   0 |     0  | ğŸ”‘  ?  |     0  |     0  |     0 \n",
      "  --------------------------------------------------------------------------------\n",
      "2    -64  |   -65  |   -66  | ğŸ”¥ -19 |   -20  |   -21  |   -20  |   -19  |   -18 \n",
      "\n",
      "Relative future reward with key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0      0  |     0  |     0  |     0  |     0  |     0  |     0  | ğŸšª   0 |   -24 \n",
      "  --------------------------------------------------------------------------------\n",
      "1    -50  | ğŸ¤  -50 |   -50  | ğŸ”¥   0 |     0  | .    0 |     0  |     0  |   -23 \n",
      "  --------------------------------------------------------------------------------\n",
      "2    -50  |   -50  |   -50  | ğŸ”¥   0 |     0  |     0  |     0  |     0  |   -22 \n",
      "\n",
      "\n",
      "Average reward: -23.33 (optimal: -6.44).\n"
     ]
    }
   ],
   "source": [
    "xs, ys = Grid().generate_data(nrof_rollouts=1000)\n",
    "dagger_policies = []\n",
    "dagger_rewards = []\n",
    "\n",
    "print(\"Collected {} data points.\".format(len(xs)))\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\n",
    "clf = clf.fit(xs, ys)\n",
    "# nodes = tree.plot_tree(clf)\n",
    "dagger_policies.append(lambda state: clf.predict([state])[0])\n",
    "\n",
    "grid = Grid((1,1),1,0)\n",
    "policy = lambda state: clf.predict([state])[0]\n",
    "grid.print_policy(policy);\n",
    "\n",
    "curr_dagger_reward_fn = rew_grid.get_reward_function(policy=dagger_policies[-1], suppress_print=True)\n",
    "rew_grid.add_key(remove=True).print_reward_fn(curr_dagger_reward_fn, show_diff=True)\n",
    "\n",
    "average_reward = rew_grid.get_average_reward(policy=dagger_policies[-1], suppress_print=True)\n",
    "optimal_reward = rew_grid.average_reward\n",
    "print(\"\\n\\nAverage reward: {:.2f} (optimal: {:.2f}).\".format(average_reward, optimal_reward))\n",
    "\n",
    "node_text = \"\\nNodes:\\n\"\n",
    "node_fls = [node.get_text().split(\"\\n\")[0] for node in nodes]\n",
    "node_text += \"\\n\".join([node_fl for node_fl in node_fls if node_fl[0] != \"g\"])\n",
    "# print(node_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second: Train with data obtained following old policy. **Run folling cell multiple times.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing another step of dataset aggregation (Done so far: 6)\n",
      "Extended dataset to size 99201.\n",
      "\n",
      "Policy without key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†“ |    â† | ğŸšª â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "1     â†’ | ğŸ¤  â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ | ğŸ”‘ â†“ |    â† |    â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "2     â†’ |    â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ |    â†‘ |    â† |    â† |    â†\n",
      "\n",
      "Policy with key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ | ğŸšª â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "1     â†’ | ğŸ¤  â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ | .  â†’ |    â†’ |    â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "2     â†’ |    â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ |    â†’ |    â†’ |    â†‘ |    â†‘\n",
      "\n",
      "Relative future reward without key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0      0  |     0  |     0  |     0  |     0  |     0  |     0  | ğŸšª   0 |     0 \n",
      "  --------------------------------------------------------------------------------\n",
      "1    -48  | ğŸ¤  -48 |   -48  | ğŸ”¥   0 |     0  | ğŸ”‘  ?  |     0  |     0  |     0 \n",
      "  --------------------------------------------------------------------------------\n",
      "2    -48  |   -48  |   -48  | ğŸ”¥   0 |     0  |     0  |     0  |     0  |     0 \n",
      "\n",
      "Relative future reward with key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0      0  |     0  |     0  |     0  |     0  |     0  |     0  | ğŸšª   0 |   -24 \n",
      "  --------------------------------------------------------------------------------\n",
      "1    -50  | ğŸ¤  -50 |   -50  | ğŸ”¥   0 |     0  | .    0 |     0  |     0  |   -23 \n",
      "  --------------------------------------------------------------------------------\n",
      "2    -50  |   -50  |   -50  | ğŸ”¥   0 |     0  |     0  |     0  |     0  |   -22 \n",
      "\n",
      "\n",
      "Average reward: -17.11 (optimal: -6.44).\n"
     ]
    }
   ],
   "source": [
    "print(\"Doing another step of dataset aggregation (Done so far: {})\".format(len(dagger_policies)))\n",
    "\n",
    "new_xs, new_ys = Grid().generate_data(policy=dagger_policies[-1], nrof_rollouts=1000)\n",
    "xs += new_xs\n",
    "ys += new_ys\n",
    "print(\"Extended dataset to size {}.\".format(len(xs)))    \n",
    "\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\n",
    "clf = clf.fit(xs, ys)\n",
    "# nodes = tree.plot_tree(clf)\n",
    "dagger_policies.append(lambda state: clf.predict([state])[0])\n",
    "Grid().print_policy(dagger_policies[-1])\n",
    "\n",
    "curr_dagger_reward_fn = rew_grid.get_reward_function(policy=dagger_policies[-1], suppress_print=True)\n",
    "rew_grid.add_key(remove=True).print_reward_fn(curr_dagger_reward_fn, show_diff=True)\n",
    "\n",
    "# dagger_policies.append(lambda state: clf.predict([state])[0])\n",
    "average_reward = rew_grid.get_average_reward(policy=dagger_policies[-1], suppress_print=True)\n",
    "optimal_reward = rew_grid.average_reward\n",
    "print(\"\\n\\nAverage reward: {:.2f} (optimal: {:.2f}).\".format(average_reward, optimal_reward))\n",
    "\n",
    "node_text = \"\\nNodes:\\n\"\n",
    "node_fls = [node.get_text().split(\"\\n\")[0] for node in nodes]\n",
    "node_text += \"\\n\".join([node_fl for node_fl in node_fls if node_fl[0] != \"g\"])\n",
    "# print(node_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r1: -23.3\n",
      "r2: -25.6\n",
      "r3: -23.3\n",
      "r4: -38.0\n",
      "r5: -26.9\n",
      "r6: -8.6\n",
      "r7: -17.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, policy in enumerate(dagger_policies):\n",
    "    # print(rew_grid.get_average_reward(policy=policy, suppress_print=True))\n",
    "    print(\"r{}: {:.1f}\".format(i+1, rew_grid.get_average_reward(policy=policy, suppress_print=True)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viper\n",
    "### Learn basic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 6390 data points.\n",
      "\n",
      "Policy without key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†“ |    â† | ğŸšª â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "1     â†’ | ğŸ¤  â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ | ğŸ”‘ â†“ |    â† |    â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "2     â†’ |    â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ |    â†“ |    â† |    â† |    â†\n",
      "\n",
      "Policy with key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ | ğŸšª â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "1     â†’ | ğŸ¤  â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ | .  â†’ |    â†’ |    â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "2     â†’ |    â†’ |    â†’ | ğŸ”¥ â†’ |    â†’ |    â†’ |    â†’ |    â†‘ |    â†‘\n",
      "\n",
      "Relative future reward without key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0      0  |     0  |     0  |     0  |     0  |     0  |     0  | ğŸšª   0 |     0 \n",
      "  --------------------------------------------------------------------------------\n",
      "1    -48  | ğŸ¤  -48 |   -48  | ğŸ”¥   0 |     0  | ğŸ”‘  ?  |     0  |     0  |     0 \n",
      "  --------------------------------------------------------------------------------\n",
      "2    -64  |   -65  |   -66  | ğŸ”¥ -19 |   -20  |   -21  |   -20  |   -19  |   -18 \n",
      "\n",
      "Relative future reward with key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0      0  |     0  |     0  |     0  |     0  |     0  |     0  | ğŸšª   0 |   -24 \n",
      "  --------------------------------------------------------------------------------\n",
      "1    -50  | ğŸ¤  -50 |   -50  | ğŸ”¥   0 |     0  | .    0 |     0  |     0  |   -23 \n",
      "  --------------------------------------------------------------------------------\n",
      "2    -50  |   -50  |   -50  | ğŸ”¥   0 |     0  |     0  |     0  |     0  |   -22 \n",
      "\n",
      "\n",
      "Average reward: -23.33 (optimal: -6.44).\n"
     ]
    }
   ],
   "source": [
    "xs, ys = Grid().generate_data(nrof_rollouts=1000)\n",
    "print(\"Collected {} data points.\".format(len(xs)))\n",
    "\n",
    "viper_policies = []\n",
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\n",
    "clf = clf.fit(xs, ys)\n",
    "clfs = [clf]\n",
    "# nodes = tree.plot_tree(clf)\n",
    "viper_policies.append(lambda state: clf.predict([state])[0])\n",
    "Grid().print_policy(viper_policies[-1])\n",
    "\n",
    "curr_viper_reward_fn = rew_grid.add_key(remove=True).get_reward_function(policy=viper_policies[-1], suppress_print=True)\n",
    "rew_grid.add_key(remove=True).print_reward_fn(curr_viper_reward_fn, show_diff=True)\n",
    "# rew_grid.add_key(remove=True).print_reward_fn(curr_viper_reward_fn, show_diff=False)\n",
    "\n",
    "average_reward = rew_grid.get_average_reward(policy=viper_policies[-1], suppress_print=True)\n",
    "optimal_reward = rew_grid.average_reward\n",
    "print(\"\\n\\nAverage reward: {:.2f} (optimal: {:.2f}).\".format(average_reward, optimal_reward))\n",
    "\n",
    "node_text = \"\\nNodes:\\n\"\n",
    "node_fls = [node.get_text().split(\"\\n\")[0] for node in nodes]\n",
    "node_text += \"\\n\".join([node_fl for node_fl in node_fls if node_fl[0] != \"g\"])\n",
    "# print(node_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from states according to l-function:\n",
    "**Run the next two cells multiple times!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69742 different states in dataset of size 31.\n",
      "\n",
      "l-function without key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0       2 |      2 |      2 |     50 |      2 |      2 |      2 | ğŸšª   2 |      1\n",
      "  --------------------------------------------------------------------------------\n",
      "1       2 | ğŸ¤    2 |     48 | ğŸ”¥  52 |     52 | ğŸ”‘  X  |      2 |      2 |      2\n",
      "  --------------------------------------------------------------------------------\n",
      "2       1 |      2 |     48 | ğŸ”¥  51 |     52 |      2 |      2 |      2 |      1\n",
      "\n",
      "l-function with key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0      X  |     X  |     X  |     X  |     X  |      2 |     X  | ğŸšª  X  |     X \n",
      "  --------------------------------------------------------------------------------\n",
      "1      X  | ğŸ¤   X  |     X  | ğŸ”¥  X  |     X  | .    2 |      2 |      2 |      2\n",
      "  --------------------------------------------------------------------------------\n",
      "2      X  |     X  |     X  | ğŸ”¥  X  |     X  |     X  |     X  |     X  |     X \n",
      "\n",
      "Data samples without key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0   212 |  286 |  281 | 2233 |  108 | 1044 |   42 |  235 |  120\n",
      "  --------------------------------------------------------------\n",
      "1    33 |   21 |  498 |  285 |  593 |    0 |   27 |   29 |   16\n",
      "  --------------------------------------------------------------\n",
      "2     6 |   11 |  236 |  277 |  603 |  230 |  270 |   23 |    5\n",
      "\n",
      "Data samples with key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     0 |    0 |    0 |    0 |    0 |  704 |    0 |    0 |    0\n",
      "  --------------------------------------------------------------\n",
      "1     0 |    0 |    0 |    0 |    0 |  187 |  197 |  170 | 1018\n",
      "  --------------------------------------------------------------\n",
      "2     0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0\n"
     ]
    }
   ],
   "source": [
    "# Generate Set of all states in data\n",
    "xs_set = set(xs)\n",
    "# print(\"Reduced dataset of size {} to set of size {}.\".format(len(xs), len(xs_set)))\n",
    "print(\"Found {} different states in dataset of size {}.\".format(len(xs), len(xs_set)))\n",
    "\n",
    "\n",
    "# Calculate l for those states\n",
    "l = {}\n",
    "grid = Grid()\n",
    "for state in xs_set:\n",
    "    rew = grid.reward_function(state)\n",
    "    minQ = None\n",
    "    for direction in range(4):\n",
    "        q_fn_val = Q_fn(state, direction)\n",
    "        if minQ is None:\n",
    "            minQ = q_fn_val\n",
    "        else:\n",
    "            minQ = min(minQ, q_fn_val)\n",
    "    l[state] = rew - minQ\n",
    "\n",
    "l_dict = l\n",
    "# l = lambda state: l_dict[state]\n",
    "# l_str = lambda grid_symb, state: \" {:3d} \".format(l(state)) if grid_symb==\" \" else \"{} {:3d}\".format(grid_symb, l(state))\n",
    "def l_str(grid_symb, state): \n",
    "    val_str = \"{:3d}\".format(l_dict[state]) if state in l_dict.keys() else \" X \"\n",
    "    if grid_symb==\" \":\n",
    "        return \" \" + val_str + \" \"\n",
    "    else:\n",
    "        return \"{} {}\".format(grid_symb, val_str)\n",
    "\n",
    "\n",
    "# print(l_dict)\n",
    "grid.print_fn_on_grid(l_str, fn_name=\"l-function\");\n",
    "max_l = max(*l_dict.values())\n",
    "# print(max_l)\n",
    "\n",
    "# Sample datapoint and reject wp l\n",
    "xs_dash, ys_dash = [], []\n",
    "count = {state:0 for state in Grid().all_states}\n",
    "while len(xs_dash) < 10000:\n",
    "    # cand = random.choice(list(xs_set))  # resample from unique data points\n",
    "    cand = random.choice(xs)  # resample from data\n",
    "    p = random.uniform(0,1)\n",
    "    if p < l_dict[cand]/max_l:\n",
    "        xs_dash.append(cand)\n",
    "        ys_dash.append(Grid().oracle_move(cand))\n",
    "        count[cand]+=1\n",
    "        \n",
    "count_fn = lambda symb, state: \"{:4d}\".format(count[state])\n",
    "Grid().print_fn_on_grid(count_fn, fn_name=\"Data samples\");\n",
    "        \n",
    "# print(len(xs_dash))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier from sampled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy without key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†“ |    â† | ğŸšª â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "1     â†‘ | ğŸ¤  â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ | ğŸ”‘ â†“ |    â† |    â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "2     â†‘ |    â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ |    â†“ |    â† |    â† |    â†\n",
      "\n",
      "Policy with key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â† | ğŸšª â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "1     â†‘ | ğŸ¤  â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ | .  â†’ |    â† |    â† |    â†\n",
      "  --------------------------------------------------------------\n",
      "2     â†‘ |    â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ |    â†’ |    â† |    â† |    â†\n",
      "\n",
      "Relative future reward without key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0    -16  |   -17  |   -18  |   -19  |   -20  |   -21  |   -20  | ğŸšª -19 |   -18 \n",
      "  --------------------------------------------------------------------------------\n",
      "1    -15  | ğŸ¤  -16 |   -17  | ğŸ”¥ -20 |   -21  | ğŸ”‘  ?  |   -21  |   -20  |   -19 \n",
      "  --------------------------------------------------------------------------------\n",
      "2    -14  |   -15  |   -16  | ğŸ”¥ -19 |   -20  |   -21  |   -20  |   -19  |   -18 \n",
      "\n",
      "Relative future reward with key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0    -18  |   -19  |   -20  |   -21  |   -22  |   -23  |   -24  | ğŸšª   0 |     0 \n",
      "  --------------------------------------------------------------------------------\n",
      "1    -17  | ğŸ¤  -18 |   -19  | ğŸ”¥ -20 |   -21  | .  -22 |   -23  |   -24  |   -23 \n",
      "  --------------------------------------------------------------------------------\n",
      "2    -16  |   -17  |   -18  | ğŸ”¥ -19 |   -20  |   -21  |   -22  |   -23  |   -22 \n",
      "\n",
      "\n",
      "Average reward: -25.00 (optimal: -6.44).\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\n",
    "clf = clf.fit(xs_dash, ys_dash)\n",
    "# nodes = tree.plot_tree(clf)\n",
    "# viper_policies.append(lambda state: clf.predict([state])[0])\n",
    "viper_policies.append(lambda state: clf.predict([state])[0])\n",
    "clfs.append(clf)\n",
    "Grid().print_policy(viper_policies[-1])\n",
    "\n",
    "curr_viper_reward_fn = rew_grid.get_reward_function(policy=viper_policies[-1], suppress_print=True)\n",
    "rew_grid.add_key(remove=True).print_reward_fn(curr_viper_reward_fn, show_diff=True)\n",
    "\n",
    "# dagger_policies.append(lambda state: clf.predict([state])[0])\n",
    "average_reward = rew_grid.get_average_reward(policy=viper_policies[-1], suppress_print=True)\n",
    "optimal_reward = rew_grid.average_reward\n",
    "print(\"\\n\\nAverage reward: {:.2f} (optimal: {:.2f}).\".format(average_reward, optimal_reward))\n",
    "\n",
    "node_text = \"\\nNodes:\\n\"\n",
    "node_fls = [node.get_text().split(\"\\n\")[0] for node in nodes]\n",
    "node_text += \"\\n\".join([node_fl for node_fl in node_fls if node_fl[0] != \"g\"])\n",
    "# print(node_text)\n",
    "\n",
    "new_xs, new_ys = Grid().generate_data(policy=viper_policies[-1], nrof_rollouts=1000)\n",
    "xs += new_xs\n",
    "ys += new_ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-23.333333333333332\n",
      "-15.111111111111112\n",
      "-25.0\n",
      "-25.0\n",
      "-25.0\n"
     ]
    }
   ],
   "source": [
    "for policy in viper_policies:\n",
    "    # print(policy)\n",
    "    print(rew_grid.get_average_reward(policy=policy, suppress_print=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show best policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing best viper policy: (idx 1)\n",
      "\n",
      "Policy without key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†“ |    â†“ | ğŸšª â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "1     â†‘ | ğŸ¤  â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ | ğŸ”‘ â†“ |    â†“ |    â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "2     â†‘ |    â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ |    â†“ |    â†“ |    â†‘ |    â†‘\n",
      "\n",
      "Policy with key:\n",
      "     0      1      2      3      4      5      6      7      8 \n",
      "0     â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ |    â†’ | ğŸšª â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "1     â†‘ | ğŸ¤  â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ | .  â†’ |    â†’ |    â†‘ |    â†‘\n",
      "  --------------------------------------------------------------\n",
      "2     â†‘ |    â†‘ |    â†‘ | ğŸ”¥ â†’ |    â†’ |    â†’ |    â†’ |    â†‘ |    â†‘\n",
      "\n",
      "Relative future reward without key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0      0  |     0  |     0  |     0  |     0  |     0  |   -20  | ğŸšª -19 |   -18 \n",
      "  --------------------------------------------------------------------------------\n",
      "1      0  | ğŸ¤    0 |     0  | ğŸ”¥   0 |     0  | ğŸ”‘  ?  |   -21  |   -20  |   -19 \n",
      "  --------------------------------------------------------------------------------\n",
      "2      0  |     0  |     0  | ğŸ”¥ -19 |   -20  |   -21  |   -20  |   -19  |   -18 \n",
      "\n",
      "Relative future reward with key:\n",
      "      0        1        2        3        4        5        6        7        8 \n",
      "0      0  |     0  |     0  |     0  |     0  |     0  |     0  | ğŸšª   0 |   -24 \n",
      "  --------------------------------------------------------------------------------\n",
      "1      0  | ğŸ¤    0 |     0  | ğŸ”¥   0 |     0  | .    0 |     0  |     0  |   -23 \n",
      "  --------------------------------------------------------------------------------\n",
      "2      0  |     0  |     0  | ğŸ”¥   0 |     0  |     0  |     0  |     0  |   -22 \n",
      "\n",
      "\n",
      "Average reward: -15.11 (optimal: -6.44).\n"
     ]
    }
   ],
   "source": [
    "best_pol_idx = -1\n",
    "best_pol_val = None\n",
    "for i, policy in enumerate(viper_policies):\n",
    "    pol_val = rew_grid.get_average_reward(policy=policy, suppress_print=True)\n",
    "    if best_pol_val is None or best_pol_val < pol_val:\n",
    "        best_pol_val = pol_val\n",
    "        best_pol_idx = i\n",
    "\n",
    "print(\"Showing best viper policy: (idx {})\".format(best_pol_idx))\n",
    "\n",
    "# Grid().print_policy(viper_policies[best_pol_idx]) # prints wrong policy it seems.\n",
    "pol = lambda state: clfs[best_pol_idx].predict([state])[0]\n",
    "Grid().print_policy(pol)\n",
    "\n",
    "\n",
    "best_pol_reward_fn = rew_grid.get_reward_function(policy=viper_policies[best_pol_idx], suppress_print=True)\n",
    "rew_grid.add_key(remove=True).print_reward_fn(best_pol_reward_fn, show_diff=True)\n",
    "\n",
    "print(\"\\n\\nAverage reward: {:.2f} (optimal: {:.2f}).\".format(best_pol_val, optimal_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_tf",
   "language": "python",
   "name": "python_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
